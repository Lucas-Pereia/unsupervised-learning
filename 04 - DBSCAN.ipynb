{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - DBSCAN\n",
    "\n",
    "## Introdução\n",
    "\n",
    "O **DBSCAN** (Density-Based Spatial Clustering of Applications with Noise) é um algoritmo de clustering baseado em densidade desenvolvido por Martin Ester e colaboradores em 1996. Diferentemente do K-Means e clustering hierárquico, o DBSCAN não requer que especifiquemos o número de clusters antecipadamente e é capaz de identificar clusters de formas arbitrárias e detectar outliers (ruído).\n",
    "\n",
    "### Características Principais do DBSCAN:\n",
    "\n",
    "1. **Baseado em densidade**: Agrupa pontos que estão densamente empacotados\n",
    "2. **Detecção de ruído**: Identifica automaticamente outliers\n",
    "3. **Forma arbitrária**: Pode encontrar clusters de qualquer formato\n",
    "4. **Número automático de clusters**: Não precisa especificar k antecipadamente\n",
    "5. **Robusto a outliers**: Outliers não afetam a formação dos clusters\n",
    "\n",
    "## Fundamentos Matemáticos\n",
    "\n",
    "O DBSCAN utiliza dois parâmetros principais:\n",
    "- $\\varepsilon$ (eps): raio da vizinhança\n",
    "- $\\text{minPts}$: número mínimo de pontos para formar um cluster\n",
    "\n",
    "### Definições Fundamentais:\n",
    "\n",
    "**1. Vizinhança-$\\varepsilon$**: Para um ponto $p$, sua vizinhança-$\\varepsilon$ é definida como:\n",
    "$$N_\\varepsilon(p) = \\{q \\in D | \\text{dist}(p,q) \\leq \\varepsilon\\}$$\n",
    "\n",
    "**2. Ponto Central (Core Point)**: Um ponto $p$ é um ponto central se:\n",
    "$$|N_\\varepsilon(p)| \\geq \\text{minPts}$$\n",
    "\n",
    "**3. Diretamente Alcançável por Densidade**: Um ponto $q$ é diretamente alcançável por densidade a partir de $p$ se:\n",
    "- $q \\in N_\\varepsilon(p)$ e\n",
    "- $p$ é um ponto central\n",
    "\n",
    "**4. Alcançável por Densidade**: Um ponto $q$ é alcançável por densidade a partir de $p$ se existe uma cadeia de pontos $p_1, p_2, ..., p_n$ onde $p_1 = p$ e $p_n = q$, tal que $p_{i+1}$ é diretamente alcançável por densidade a partir de $p_i$.\n",
    "\n",
    "**5. Conectado por Densidade**: Dois pontos $p$ e $q$ são conectados por densidade se existe um ponto $o$ tal que tanto $p$ quanto $q$ são alcançáveis por densidade a partir de $o$.\n",
    "\n",
    "### Classificação dos Pontos:\n",
    "\n",
    "- **Core Point (Ponto Central)**: $|N_\\varepsilon(p)| \\geq \\text{minPts}$\n",
    "- **Border Point (Ponto de Fronteira)**: $|N_\\varepsilon(p)| < \\text{minPts}$, mas está na vizinhança de um core point\n",
    "- **Noise Point (Ponto de Ruído)**: Não é core nem border point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN as SklearnDBSCAN\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.stats import mode\n",
    "import pandas as pd\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementação do DBSCAN\n",
    "\n",
    "Vamos implementar o algoritmo DBSCAN passo a passo usando apenas NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DBSCAN:\n",
    "    def __init__(self, eps=0.5, min_pts=5, metric='euclidean'):\n",
    "        \"\"\"Inicializa o DBSCAN com os parâmetros eps e min_pts\"\"\"\n",
    "        self.eps = eps\n",
    "        self.min_pts = min_pts\n",
    "        self.metric = metric\n",
    "        self.labels_ = None\n",
    "        self.core_samples_ = None\n",
    "        self.n_clusters_ = 0\n",
    "    \n",
    "    def _calculate_distance_matrix(self, X):\n",
    "        \"\"\"Calcula a matriz de distâncias entre todos os pontos\"\"\"\n",
    "        if self.metric == 'euclidean':\n",
    "            distances = np.linalg.norm(X[:, np.newaxis] - X, axis=2)\n",
    "        # elif self.metric == '...':\n",
    "            # distance = ...\n",
    "        else:\n",
    "            raise ValueError(\"Métrica não suportada\")\n",
    "        return distances\n",
    "    \n",
    "    def _get_neighbors(self, point_idx, distance_matrix):\n",
    "        \"\"\"Encontra todos os vizinhos dentro da distância eps\"\"\"\n",
    "        return np.where(distance_matrix[point_idx] <= self.eps)[0]\n",
    "    \n",
    "    def _expand_cluster(self, point_idx, neighbors, cluster_id, distance_matrix, visited, labels):\n",
    "        \"\"\"Expande o cluster a partir do ponto inicial\"\"\"\n",
    "        labels[point_idx] = cluster_id\n",
    "        queue = neighbors.tolist()\n",
    "\n",
    "        while queue:\n",
    "            neighbor_idx = queue.pop()\n",
    "\n",
    "            if not visited[neighbor_idx]:\n",
    "                visited[neighbor_idx] = True\n",
    "                neighbor_neighbors = self._get_neighbors(neighbor_idx, distance_matrix)\n",
    "\n",
    "                if len(neighbor_neighbors) >= self.min_pts:\n",
    "                    queue.extend(neighbor_neighbors)\n",
    "\n",
    "            if labels[neighbor_idx] == -1:\n",
    "                labels[neighbor_idx] = cluster_id\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"Executa o algoritmo DBSCAN\"\"\"\n",
    "        n_points = len(X)\n",
    "        visited = np.zeros(n_points, dtype=bool)\n",
    "        cluster_id = 0\n",
    "        self.labels_ = np.full(n_points, -1)  # -1 = ruído\n",
    "        self.core_samples_ = []\n",
    "\n",
    "        distance_matrix = self._calculate_distance_matrix(X)\n",
    "\n",
    "        for point_idx in range(n_points):\n",
    "            if visited[point_idx]:\n",
    "                continue\n",
    "\n",
    "            visited[point_idx] = True\n",
    "            neighbors = self._get_neighbors(point_idx, distance_matrix)\n",
    "\n",
    "            if len(neighbors) >= self.min_pts:   # core point\n",
    "                self.core_samples_.append(point_idx)\n",
    "                self._expand_cluster(point_idx, neighbors, cluster_id, distance_matrix, visited, self.labels_)\n",
    "                cluster_id += 1\n",
    "\n",
    "        self.core_samples_ = np.array(self.core_samples_)\n",
    "        self.n_clusters_ = cluster_id\n",
    "        return self\n",
    "\n",
    "    def fit_predict(self, X):\n",
    "        \"\"\"Executa DBSCAN e retorna os labels\"\"\"\n",
    "        self.fit(X)\n",
    "        return self.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstração com Dados Sintéticos\n",
    "\n",
    "Vamos criar dados sintéticos para demonstrar o funcionamento do DBSCAN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(42)\n",
    "\n",
    "# Cabeça: círculo com leve ruído\n",
    "n_head = 400\n",
    "theta = rng.uniform(0, 2*np.pi, n_head)\n",
    "R = 10 + rng.normal(0, 0.35, n_head)\n",
    "head = np.c_[R*np.cos(theta), R*np.sin(theta)]\n",
    "y_head = np.full(n_head, 0)\n",
    "\n",
    "# Olhos: dois blobs gaussianos\n",
    "n_eye = 100\n",
    "eye_left  = rng.normal(loc=[-3.2,  3.0], scale=[0.45, 0.45], size=(n_eye//2, 2))\n",
    "eye_right = rng.normal(loc=[ 3.2,  3.0], scale=[0.45, 0.45], size=(n_eye - n_eye//2, 2))\n",
    "eyes = np.vstack([eye_left, eye_right])\n",
    "y_eyes = np.full(eyes.shape[0], 1)\n",
    "\n",
    "# Boca: arco inferior com jitter (sorriso)\n",
    "n_mouth = 100\n",
    "phi = rng.uniform(np.deg2rad(200), np.deg2rad(340), n_mouth)  # arco de 200° a 340°\n",
    "Rm = 5 + rng.normal(0, 0.22, n_mouth)\n",
    "mouth = np.c_[Rm*np.cos(phi), -1 + Rm*np.sin(phi)]\n",
    "mouth += rng.normal(0, [0.12, 0.15], mouth.shape)  # engrossar um pouco\n",
    "y_mouth = np.full(n_mouth, 2)\n",
    "\n",
    "# Ruído: pontos aleatórios\n",
    "n_noise = 100\n",
    "noise = rng.uniform(low=[-13, -13], high=[13, 13], size=(n_noise, 2))\n",
    "y_noise = np.full(n_noise, -1)\n",
    "\n",
    "# Concatenar\n",
    "X_synthetic = np.vstack([head, eyes, mouth, noise])\n",
    "true_labels  = np.concatenate([y_head, y_eyes, y_mouth, y_noise])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar os dados sintéticos\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "colors = ['red', 'blue', 'green', 'gray']\n",
    "for i in range(4):\n",
    "    if i == 3:  # ruído\n",
    "        mask = true_labels == -1\n",
    "        plt.scatter(X_synthetic[mask, 0], X_synthetic[mask, 1], c=colors[i], alpha=0.6, s=30, marker='x', label='Ruído')\n",
    "    else:\n",
    "        mask = true_labels == i\n",
    "        plt.scatter(X_synthetic[mask, 0], X_synthetic[mask, 1], c=colors[i], alpha=0.7, s=50, label=f'Cluster {i+1}')\n",
    "\n",
    "plt.title('Dados Sintéticos - Classes Verdadeiras')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_synthetic[:, 0], X_synthetic[:, 1], c='black', alpha=0.6, s=30)\n",
    "plt.title('Dados Sintéticos - Sem Labels')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(eps=1.3, min_pts=7)\n",
    "labels = dbscan.fit_predict(X_synthetic)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "unique_labels = np.unique(labels)\n",
    "colors = plt.cm.Set1(np.linspace(0, 1, len(unique_labels)))\n",
    "\n",
    "for k, label in enumerate(unique_labels):\n",
    "    if label == -1:\n",
    "        # Ruído\n",
    "        mask = labels == label\n",
    "        plt.scatter(X_synthetic[mask, 0], X_synthetic[mask, 1], c='black', marker='x', s=30, alpha=0.6, label='Ruído')\n",
    "    else:\n",
    "        # Clusters\n",
    "        mask = labels == label\n",
    "        plt.scatter(X_synthetic[mask, 0], X_synthetic[mask, 1], c=[colors[k]], s=50, alpha=0.7, label=f'Cluster {label}')\n",
    "\n",
    "# Destacar core points\n",
    "if len(dbscan.core_samples_) > 0:\n",
    "    plt.scatter(X_synthetic[dbscan.core_samples_, 0], \n",
    "                X_synthetic[dbscan.core_samples_, 1],\n",
    "                s=100, facecolors='none', edgecolors='black', \n",
    "                linewidth=2, alpha=0.8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicando DBSCAN aos Dados Sintéticos\n",
    "\n",
    "Agora vamos aplicar nosso algoritmo DBSCAN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testar diferentes valores de eps e min_pts\n",
    "eps_values = [0.5, 1.3, 2.0]\n",
    "min_pts_values = [3, 5, 8]\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
    "fig.suptitle('DBSCAN com Diferentes Parâmetros', fontsize=16)\n",
    "\n",
    "for i, eps in enumerate(eps_values):\n",
    "    for j, min_pts in enumerate(min_pts_values):\n",
    "        # Aplicar DBSCAN\n",
    "        dbscan = DBSCAN(eps=eps, min_pts=min_pts)\n",
    "        labels = dbscan.fit_predict(X_synthetic)\n",
    "        \n",
    "        # Visualizar resultados\n",
    "        ax = axes[i, j]\n",
    "        \n",
    "        unique_labels = np.unique(labels)\n",
    "        colors = plt.cm.Set1(np.linspace(0, 1, len(unique_labels)))\n",
    "        \n",
    "        for k, label in enumerate(unique_labels):\n",
    "            if label == -1:\n",
    "                # Ruído\n",
    "                mask = labels == label\n",
    "                ax.scatter(X_synthetic[mask, 0], X_synthetic[mask, 1], \n",
    "                          c='black', marker='x', s=30, alpha=0.6, label='Ruído')\n",
    "            else:\n",
    "                # Clusters\n",
    "                mask = labels == label\n",
    "                ax.scatter(X_synthetic[mask, 0], X_synthetic[mask, 1], \n",
    "                          c=[colors[k]], s=50, alpha=0.7, label=f'Cluster {label}')\n",
    "        \n",
    "        # Destacar core points\n",
    "        if len(dbscan.core_samples_) > 0:\n",
    "            ax.scatter(X_synthetic[dbscan.core_samples_, 0], \n",
    "                      X_synthetic[dbscan.core_samples_, 1],\n",
    "                      s=100, facecolors='none', edgecolors='black', \n",
    "                      linewidth=2, alpha=0.8)\n",
    "        \n",
    "        ax.set_title(f'eps={eps}, min_pts={min_pts}\\n{dbscan.n_clusters_} clusters')\n",
    "        ax.set_xlabel('Feature 1')\n",
    "        ax.set_ylabel('Feature 2')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análise com Parâmetros Ótimos\n",
    "\n",
    "Vamos escolher os parâmetros que melhor capturam a estrutura dos dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parâmetros que parecem funcionar melhor\n",
    "best_eps = 1.3\n",
    "best_min_pts = 5\n",
    "\n",
    "# Aplicar DBSCAN com os melhores parâmetros\n",
    "dbscan_best = DBSCAN(eps=best_eps, min_pts=best_min_pts)\n",
    "labels_best = dbscan_best.fit_predict(X_synthetic)\n",
    "\n",
    "# Análise detalhada\n",
    "unique_labels = np.unique(labels_best)\n",
    "n_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)\n",
    "n_noise = np.sum(labels_best == -1)\n",
    "n_core_samples = len(dbscan_best.core_samples_)\n",
    "\n",
    "print(f\"Resultados do DBSCAN (eps={best_eps}, min_pts={best_min_pts}):\")\n",
    "print(f\"- Número de clusters encontrados: {n_clusters}\")\n",
    "print(f\"- Número de pontos de ruído: {n_noise}\")\n",
    "print(f\"- Número de core samples: {n_core_samples}\")\n",
    "print(f\"- Labels únicos: {unique_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classificar pontos por tipo\n",
    "core_mask = np.zeros(len(X_synthetic), dtype=bool)\n",
    "if len(dbscan_best.core_samples_) > 0:\n",
    "    core_mask[dbscan_best.core_samples_] = True\n",
    "\n",
    "border_mask = (labels_best != -1) & (~core_mask)\n",
    "noise_mask = labels_best == -1\n",
    "\n",
    "print(f\"Classificação dos pontos:\")\n",
    "print(f\"- Core points: {np.sum(core_mask)}\")\n",
    "print(f\"- Border points: {np.sum(border_mask)}\")\n",
    "print(f\"- Noise points: {np.sum(noise_mask)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualização detalhada dos tipos de pontos\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Subplot 1: Clusters encontrados\n",
    "plt.subplot(1, 3, 1)\n",
    "colors = plt.cm.Set1(np.linspace(0, 1, max(len(unique_labels), 3)))\n",
    "\n",
    "for i, label in enumerate(unique_labels):\n",
    "    if label == -1:\n",
    "        mask = labels_best == label\n",
    "        plt.scatter(X_synthetic[mask, 0], X_synthetic[mask, 1], \n",
    "                   c='black', marker='x', s=50, alpha=0.8, label='Ruído')\n",
    "    else:\n",
    "        mask = labels_best == label\n",
    "        plt.scatter(X_synthetic[mask, 0], X_synthetic[mask, 1], \n",
    "                   c=[colors[i]], s=60, alpha=0.8, label=f'Cluster {label}')\n",
    "\n",
    "plt.title(f'DBSCAN - Clusters Encontrados\\n{n_clusters} clusters, {n_noise} ruído')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 2: Tipos de pontos\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(X_synthetic[core_mask, 0], X_synthetic[core_mask, 1], \n",
    "           c='red', s=80, alpha=0.8, label='Core Points', marker='o')\n",
    "plt.scatter(X_synthetic[border_mask, 0], X_synthetic[border_mask, 1], \n",
    "           c='blue', s=60, alpha=0.8, label='Border Points', marker='s')\n",
    "plt.scatter(X_synthetic[noise_mask, 0], X_synthetic[noise_mask, 1], \n",
    "           c='black', s=40, alpha=0.8, label='Noise Points', marker='x')\n",
    "\n",
    "plt.title('Classificação dos Pontos')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 3: Comparação com ground truth\n",
    "plt.subplot(1, 3, 3)\n",
    "for i in range(4):\n",
    "    if i == 3:  # ruído\n",
    "        mask = true_labels == -1\n",
    "        plt.scatter(X_synthetic[mask, 0], X_synthetic[mask, 1], \n",
    "                   c='gray', alpha=0.6, s=30, marker='x', label='Ruído Real')\n",
    "    else:\n",
    "        mask = true_labels == i\n",
    "        plt.scatter(X_synthetic[mask, 0], X_synthetic[mask, 1], \n",
    "                   c=colors[i], alpha=0.7, s=50, label=f'Cluster Real {i+1}')\n",
    "\n",
    "plt.title('Ground Truth')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimativa do Parâmetro $\\varepsilon$ usando K-Distance\n",
    "\n",
    "Uma das maiores dificuldades do DBSCAN é escolher o valor apropriado para o parâmetro $\\varepsilon$ (eps). O método **K-Distance** é uma heurística eficaz para estimar este parâmetro.\n",
    "\n",
    "O método K-Distance consiste em:\n",
    "\n",
    "1. **Calcular a k-ésima distância mais próxima** para cada ponto no dataset\n",
    "2. **Ordenar essas distâncias** em ordem decrescente\n",
    "3. **Identificar o \"cotovelo\"** no gráfico resultante\n",
    "\n",
    "A intuição é que pontos dentro de clusters densos terão k-ésimas distâncias pequenas, enquanto pontos de ruído ou em bordas de clusters terão distâncias maiores.\n",
    "\n",
    "### Algoritmo K-Distance:\n",
    "\n",
    "Para um dataset $D$ e parâmetro $k = \\text{minPts} - 1$:\n",
    "\n",
    "1. Para cada ponto $p_i \\in D$:\n",
    "   - Calcule $d_k(p_i)$ = distância ao k-ésimo vizinho mais próximo\n",
    "2. Ordene os valores $d_k(p_i)$\n",
    "3. Plote o gráfico K-Distance\n",
    "4. Escolha $\\varepsilon$ no ponto onde a curva tem maior curvatura (cotovelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def plot_k_distance(X, min_pts, title=\"K-Distance Plot\"):\n",
    "    \"\"\"Plota o gráfico K-Distance usando sklearn.NearestNeighbors.\"\"\"\n",
    "    k = int(min_pts - 1)\n",
    "\n",
    "    nn = NearestNeighbors(n_neighbors=k+1, metric=\"euclidean\")\n",
    "    nn.fit(X)\n",
    "    distances, _ = nn.kneighbors(X)\n",
    "\n",
    "    kth_distances = distances[:, k]\n",
    "    k_distances_sorted = np.sort(kth_distances)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(len(k_distances_sorted)), k_distances_sorted, linewidth=2, label=f'{k}-distance')\n",
    "    plt.xlabel(\"Pontos ordenados por distância\")\n",
    "    plt.ylabel(f\"{k}-distance\")\n",
    "    plt.title(title)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_k_distance(X_synthetic, min_pts=5, title=\"K-Distance Plot para Dados Sintéticos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vantagens e Desvantagens do DBSCAN\n",
    "\n",
    "### Vantagens:\n",
    "\n",
    "1. **Não requer especificar o número de clusters antecipadamente**\n",
    "2. **Pode encontrar clusters de forma arbitrária** (não apenas esféricos)\n",
    "3. **Identifica automaticamente outliers/ruído**\n",
    "4. **Robusto a outliers** (não afetam a formação dos clusters)\n",
    "5. **Determinístico** (sempre produz os mesmos resultados)\n",
    "\n",
    "### Desvantagens:\n",
    "\n",
    "1. **Sensível aos parâmetros** eps e min_pts\n",
    "2. **Dificuldade com clusters de densidades diferentes**\n",
    "3. **Problemas em alta dimensionalidade** (\"curse of dimensionality\")\n",
    "4. **Complexidade computacional** O(n²) no pior caso\n",
    "5. **Requer escolha cuidadosa da métrica de distância**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercícios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 1: Ajuste de Parâmetros no DBSCAN em 3D\n",
    "\n",
    "Com os dados das **três esferas concêntricas**, realize:\n",
    "\n",
    "1. Plotar o K-Distance para diferentes valores de `min_pts` e sugerir um intervalo adequado para `eps`.\n",
    "2. Selecionar os melhores parâmetros de `min_pts` e `eps`.\n",
    "3. Visualizar em 3D os clusters encontrados (cores diferentes) e comentar a escolha de `eps` e `min_samples`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "\n",
    "def k_distance(X, k):\n",
    "    nbrs = NearestNeighbors(n_neighbors=k).fit(X)\n",
    "    dists, _ = nbrs.kneighbors(X)\n",
    "    return np.sort(dists[:, -1])\n",
    "\n",
    "def plot_kdist_for_minpts(X, minpts_list):\n",
    "    plt.figure(figsize=(8,5))\n",
    "    for k in minpts_list:\n",
    "        kd = k_distance(X, k)\n",
    "        plt.plot(kd, label=f'min_pts={k}', linewidth=1.6)\n",
    "    plt.title(\"K-Distance para diferentes min_pts\")\n",
    "    plt.xlabel(\"Pontos ordenados\")\n",
    "    plt.ylabel(\"Distância\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=.3)\n",
    "    plt.show()\n",
    "\n",
    "# Faixa típica em 3D: min_pts entre 6 e 12\n",
    "plot_kdist_for_minpts(X_spheres, [6, 8, 10, 12])\n",
    "\n",
    "# Sugestão inicial de eps pelo cotovelo\n",
    "def knee_interval(kdist, low_q=0.85, high_q=0.95):\n",
    "    return np.quantile(kdist, low_q), np.quantile(kdist, high_q)\n",
    "\n",
    "kdist_ref = k_distance(X_spheres, 10)\n",
    "eps_lo, eps_hi = knee_interval(kdist_ref)\n",
    "print(f\"Intervalo sugerido para eps ≈ [{eps_lo:.3f}, {eps_hi:.3f}]\")\n",
    "\n",
    "def avalia_dbscan(X, eps, min_samples):\n",
    "    model = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    labels = model.fit_predict(X)\n",
    "    n_noise = np.sum(labels == -1)\n",
    "    k = len(set(labels) - {-1})\n",
    "    return labels, k, n_noise / len(X)\n",
    "\n",
    "eps_grid = np.linspace(eps_lo, eps_hi, 11)\n",
    "minpts_grid = [6, 8, 10, 12]\n",
    "\n",
    "resultados = []\n",
    "for m in minpts_grid:\n",
    "    for e in eps_grid:\n",
    "        labels, k, noise = avalia_dbscan(X_spheres, e, m)\n",
    "        resultados.append((m, e, k, noise))\n",
    "\n",
    "resultados_ordenados = sorted(resultados, key=lambda t: (abs(t[2]-3), t[3], t[1]))\n",
    "best_min, best_eps, best_k, best_noise = resultados_ordenados[0]\n",
    "labels_best, _, _ = avalia_dbscan(X_spheres, best_eps, best_min)\n",
    "\n",
    "print(f\"Melhor escolha: min_samples={best_min}, eps={best_eps:.3f} → clusters={best_k}, ruído={100*best_noise:.1f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "df_plot = pd.DataFrame(X_spheres, columns=[\"x\",\"y\",\"z\"])\n",
    "df_plot[\"cluster\"] = labels_best.astype(str)\n",
    "\n",
    "fig = px.scatter_3d(\n",
    "    df_plot, x=\"x\", y=\"y\", z=\"z\",\n",
    "    color=\"cluster\",\n",
    "    color_discrete_sequence=px.colors.qualitative.Vivid\n",
    ")\n",
    "fig.update_traces(marker=dict(size=3))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 2: DBSCAN com distância radial\n",
    "\n",
    "Usando os dados das **3 esferas concêntricas** do exercício anterior:\n",
    "\n",
    "1. Implemente a **distância radial** e use-a no DBSCAN. A **distância radial** entre dois pontos \\(x_i\\) e \\(x_j\\) é a diferença absoluta entre suas distâncias à origem: $d_{\\text{radial}}(x_i, x_j) = \\big|\\;\\|x_i\\|_2 - \\|x_j\\|_2\\;\\big|$\n",
    "2. Plote o **K-Distance radial** para sugerir `eps`.  \n",
    "3. Teste combinações de `eps` e `min_samples`.  \n",
    "4. Visualize em 3D os clusters obtidos e compare com o resultado usando distância euclidiana.  \n",
    "5. Comente brevemente qual configuração foi melhor e por quê a métrica radial ajuda nesse dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import DBSCAN\n",
    "import plotly.express as px\n",
    "\n",
    "assert 'X_spheres' in globals(), \"Execute antes o bloco do Exercício 1 que define X_spheres (padronizado).\"\n",
    "\n",
    "# ---------- Definições ----------\n",
    "def radial_metric(u, v):\n",
    "    # d_radial(u,v) = | ||u||_2 - ||v||_2 |\n",
    "    return abs(np.linalg.norm(u) - np.linalg.norm(v))\n",
    "\n",
    "def k_distance_custom(X, k, metric, algorithm='brute'):\n",
    "    nn = NearestNeighbors(n_neighbors=k, metric=metric, algorithm=algorithm)\n",
    "    nn.fit(X)\n",
    "    dists, _ = nn.kneighbors(X)\n",
    "    return np.sort(dists[:, -1])\n",
    "\n",
    "def knee_interval(kdist, low_q=0.85, high_q=0.95):\n",
    "    return np.quantile(kdist, low_q), np.quantile(kdist, high_q)\n",
    "\n",
    "def avalia_dbscan(X, eps, min_samples, metric):\n",
    "    mdl = DBSCAN(eps=eps, min_samples=min_samples, metric=metric)\n",
    "    labels = mdl.fit_predict(X)\n",
    "    n_noise = int(np.sum(labels == -1))\n",
    "    k = len(set(labels) - {-1})\n",
    "    noise_rate = n_noise / len(X)\n",
    "    return labels, k, noise_rate\n",
    "\n",
    "# ---------- 1) K-Distance (métrica radial) e sugestão de eps ----------\n",
    "minpts_list = [6, 8, 10, 12]  # faixa estável em 3D\n",
    "k_ref = 10\n",
    "\n",
    "kdist_curvas = []\n",
    "for k in minpts_list:\n",
    "    kd = k_distance_custom(X_spheres, k, metric=radial_metric)  # radial\n",
    "    kdist_curvas.append(pd.DataFrame({\"ordem\": np.arange(len(kd)), \"k_distance\": kd, \"min_pts\": str(k)}))\n",
    "\n",
    "df_kd = pd.concat(kdist_curvas, ignore_index=True)\n",
    "\n",
    "# Plot K-Distance radial (todas as curvas)\n",
    "fig_kd = px.line(\n",
    "    df_kd, x=\"ordem\", y=\"k_distance\", color=\"min_pts\",\n",
    "    title=\"K-Distance (métrica radial) para diferentes min_pts\"\n",
    ")\n",
    "fig_kd.show()\n",
    "\n",
    "# Intervalo sugerido de eps pela curva com k_ref\n",
    "kdist_ref_radial = k_distance_custom(X_spheres, k_ref, metric=radial_metric)\n",
    "eps_lo_rad, eps_hi_rad = knee_interval(kdist_ref_radial, 0.85, 0.95)\n",
    "print(f\"[RADIAL] Intervalo sugerido para eps (min_pts={k_ref}): [{eps_lo_rad:.3f}, {eps_hi_rad:.3f}]\")\n",
    "\n",
    "# ---------- 2) Grade de (eps, min_samples) — métrica radial ----------\n",
    "eps_grid_radial = np.linspace(eps_lo_rad, eps_hi_rad, 11)\n",
    "res_radial = []\n",
    "for m in minpts_list:\n",
    "    for e in eps_grid_radial:\n",
    "        labels, k, noise = avalia_dbscan(X_spheres, e, m, metric=radial_metric)\n",
    "        res_radial.append((m, float(e), int(k), float(noise)))\n",
    "res_radial_sorted = sorted(res_radial, key=lambda t: (abs(t[2]-3), t[3], t[1]))\n",
    "\n",
    "best_min_rad, best_eps_rad, best_k_rad, best_noise_rad = res_radial_sorted[0]\n",
    "labels_radial, _, _ = avalia_dbscan(X_spheres, best_eps_rad, best_min_rad, metric=radial_metric)\n",
    "\n",
    "print(\"Top 5 (RADIAL):\")\n",
    "for r in res_radial_sorted[:5]:\n",
    "    print(f\"min_samples={r[0]:>2} | eps={r[1]:.3f} | clusters={r[2]} | ruído={100*r[3]:.1f}%\")\n",
    "print(f\">>> Escolha RADIAL: min_samples={best_min_rad}, eps={best_eps_rad:.3f} \"\n",
    "      f\"→ clusters={best_k_rad}, ruído={100*best_noise_rad:.1f}%\")\n",
    "\n",
    "# ---------- 3) Baseline EUCLIDIANA (mesma lógica de busca) ----------\n",
    "# K-Distance euclidiana p/ eps\n",
    "kdist_ref_eucl = k_distance_custom(X_spheres, k_ref, metric='euclidean', algorithm='auto')\n",
    "eps_lo_eucl, eps_hi_eucl = knee_interval(kdist_ref_eucl, 0.85, 0.95)\n",
    "print(f\"[EUCLIDIANA] Intervalo sugerido para eps (min_pts={k_ref}): [{eps_lo_eucl:.3f}, {eps_hi_eucl:.3f}]\")\n",
    "\n",
    "eps_grid_eucl = np.linspace(eps_lo_eucl, eps_hi_eucl, 11)\n",
    "res_eucl = []\n",
    "for m in minpts_list:\n",
    "    for e in eps_grid_eucl:\n",
    "        labels, k, noise = avalia_dbscan(X_spheres, e, m, metric='euclidean')\n",
    "        res_eucl.append((m, float(e), int(k), float(noise)))\n",
    "res_eucl_sorted = sorted(res_eucl, key=lambda t: (abs(t[2]-3), t[3], t[1]))\n",
    "\n",
    "best_min_eucl, best_eps_eucl, best_k_eucl, best_noise_eucl = res_eucl_sorted[0]\n",
    "labels_euclid, _, _ = avalia_dbscan(X_spheres, best_eps_eucl, best_min_eucl, metric='euclidean')\n",
    "\n",
    "print(\"Top 5 (EUCLIDIANA):\")\n",
    "for r in res_eucl_sorted[:5]:\n",
    "    print(f\"min_samples={r[0]:>2} | eps={r[1]:.3f} | clusters={r[2]} | ruído={100*r[3]:.1f}%\")\n",
    "print(f\">>> Escolha EUCLIDIANA: min_samples={best_min_eucl}, eps={best_eps_eucl:.3f} \"\n",
    "      f\"→ clusters={best_k_eucl}, ruído={100*best_noise_eucl:.1f}%\")\n",
    "\n",
    "# ---------- 4) Visualizações 3D (Plotly) ----------\n",
    "# RADIAL\n",
    "df_plot_rad = pd.DataFrame(X_spheres, columns=[\"x\",\"y\",\"z\"])\n",
    "df_plot_rad[\"cluster\"] = labels_radial.astype(str)\n",
    "fig_rad = px.scatter_3d(\n",
    "    df_plot_rad, x=\"x\", y=\"y\", z=\"z\",\n",
    "    color=\"cluster\",\n",
    "    title=f\"DBSCAN (métrica radial) | eps={best_eps_rad:.3f}, min_samples={best_min_rad}\",\n",
    "    color_discrete_sequence=px.colors.qualitative.Vivid\n",
    ")\n",
    "fig_rad.update_traces(marker=dict(size=3))\n",
    "fig_rad.show()\n",
    "\n",
    "# EUCLIDIANA\n",
    "df_plot_eu = pd.DataFrame(X_spheres, columns=[\"x\",\"y\",\"z\"])\n",
    "df_plot_eu[\"cluster\"] = labels_euclid.astype(str)\n",
    "fig_eu = px.scatter_3d(\n",
    "    df_plot_eu, x=\"x\", y=\"y\", z=\"z\",\n",
    "    color=\"cluster\",\n",
    "    title=f\"DBSCAN (euclidiana) | eps={best_eps_eucl:.3f}, min_samples={best_min_eucl}\",\n",
    "    color_discrete_sequence=px.colors.qualitative.Vivid\n",
    ")\n",
    "fig_eu.update_traces(marker=dict(size=3))\n",
    "fig_eu.show()\n",
    "\n",
    "# ---------- Comentário ----------\n",
    "print(\"\\n 5) Comentário:\")\n",
    "print(\"Foi usada a distância radial no DBSCAN, que compara só o raio de cada ponto em relação à origem. Isso ajudou a separar bem as três esferas concêntricas, formando 3 clusters corretos e com pouco ruído.\")\n",
    "print(\"Quando testada a distância euclidiana, às vezes pontos de esferas diferentes acabavam juntos (se estavam próximos pelo ângulo), e isso confundia os agrupamentos. Já a métrica radial “ignora o ângulo” e olha apenas para o raio, que é justamente a característica que distingue as cascas.\")\n",
    "print(\"A melhor configuração ficou com min_samples entre 8 e 12 e eps dentro do intervalo indicado pelo gráfico de K-Distance (entre os percentis 85 e 95). Assim consegui separar os três grupos como esperado.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 3: Detecção de Anomalias com DBSCAN e DTW\n",
    "\n",
    "O **DTW (Dynamic Time Warping)** mede a similaridade entre séries temporais mesmo quando estão defasadas ou com velocidades diferentes, alinhando-as de forma elástica. Isso permite detectar padrões semelhantes sem que a defasagem atrapalhe.\n",
    "\n",
    "Pode ser calculado por:\n",
    "```python\n",
    "from dtaidistance import dtw\n",
    "\n",
    "n = len(X)              # número de séries\n",
    "D = np.zeros((n, n))    # matriz de distâncias\n",
    "\n",
    "for i in range(n):\n",
    "    for j in range(i+1, n):\n",
    "        dist = dtw.distance_fast(X[i], X[j])  # distância DTW\n",
    "        D[i, j] = D[j, i] = dist              # matriz simétrica\n",
    "````\n",
    "\n",
    "**Tarefas:**\n",
    "1. Use o dataset de senóides com variação e **anomalias simuladas**.  \n",
    "2. Adicione a métrica DTW no DBSCAN.\n",
    "3. Experimente diferentes valores de `eps` e `min_samples` até que o modelo consiga separar bem séries normais das anômalas.  \n",
    "4. Plote todas as séries, usando uma cor para as normais e outra para as anomalias detectadas (`label = -1`).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# 1) Dataset de senóides com anomalias\n",
    "def generate_time_series_dataset(n_series=50, length=100, noise=0.1, n_outliers=2, random_state=42):\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    X, y = [], []\n",
    "    t = np.linspace(0, 4*np.pi, length)\n",
    "\n",
    "    # séries normais: senóide com amplitude e frequência ligeiramente diferentes\n",
    "    for _ in range(n_series):\n",
    "        amp = rng.uniform(0.8, 1.2)         # amplitude\n",
    "        freq = rng.uniform(0.9, 1.1)        # frequência\n",
    "        phase = rng.uniform(0, 0.5*np.pi)   # pequena defasagem\n",
    "        series = amp * np.sin(freq * t + phase) + noise * rng.normal(size=length)\n",
    "        X.append(series); y.append(0)  # normal\n",
    "\n",
    "    # outliers: picos ou deslocamentos fortes\n",
    "    for _ in range(n_outliers):\n",
    "        amp = rng.uniform(1.5, 2.0)         # amplitude anômala\n",
    "        freq = rng.uniform(1.2, 1.5)        # frequência anômala\n",
    "        series = amp * np.sin(freq * t) + noise * rng.normal(size=length)\n",
    "        if rng.random() < 0.5:\n",
    "            series[length//2] += 3  # pico\n",
    "        else:\n",
    "            series += rng.normal(2.0, 0.5)  # deslocamento\n",
    "        X.append(series); y.append(-1)  # anomalia\n",
    "\n",
    "    return np.array(X), np.array(y), t\n",
    "\n",
    "X, y_true, t = generate_time_series_dataset()\n",
    "\n",
    "# 2) Distância DTW\n",
    "USING_LIB = False\n",
    "try:\n",
    "    from dtaidistance import dtw as _dtw\n",
    "    def dtw_distance(a, b):\n",
    "        return float(_dtw.distance_fast(a, b))\n",
    "    USING_LIB = True\n",
    "except Exception:\n",
    "    def dtw_distance_window(a, b, w):\n",
    "        n = len(a); m = len(b)\n",
    "        w = max(w, abs(n - m))\n",
    "        INF = 1e18\n",
    "        D = np.full((n+1, m+1), INF, dtype=float)\n",
    "        D[0, 0] = 0.0\n",
    "        for i in range(1, n+1):\n",
    "            ai = a[i-1]\n",
    "            j0 = max(1, i - w)\n",
    "            j1 = min(m, i + w)\n",
    "            for j in range(j0, j1+1):\n",
    "                cost = abs(ai - b[j-1])\n",
    "                D[i, j] = cost + min(D[i-1, j], D[i, j-1], D[i-1, j-1])\n",
    "        return float(D[n, m])\n",
    "    def dtw_distance(a, b):\n",
    "        # janela de ~10% do comprimento para acelerar\n",
    "        return dtw_distance_window(a, b, w=max(6, len(a)//10))\n",
    "\n",
    "print(\"DTW usado:\", \"dtaidistance.distance_fast\" if USING_LIB else \"DTW com janela (NumPy)\")\n",
    "\n",
    "# 3) Matriz de distâncias DTW\n",
    "n = len(X)\n",
    "D = np.zeros((n, n), dtype=float)\n",
    "for i in range(n):\n",
    "    for j in range(i+1, n):\n",
    "        d = dtw_distance(X[i], X[j])\n",
    "        D[i, j] = D[j, i] = d\n",
    "\n",
    "# 4) Busca simples de (eps, min_samples) para separar normais vs anômalas\n",
    "du = D[np.triu_indices(n, 1)]\n",
    "q70, q95 = np.quantile(du, 0.70), np.quantile(du, 0.95)\n",
    "eps_grid = np.linspace(q70, q95, 6)\n",
    "min_grid = [3, 4, 5]\n",
    "\n",
    "def f1_anom(labels, truth):\n",
    "    tp = np.sum((labels == -1) & (truth == -1))\n",
    "    fp = np.sum((labels == -1) & (truth ==  0))\n",
    "    fn = np.sum((labels != -1) & (truth == -1))\n",
    "    if tp == 0: return 0.0\n",
    "    prec = tp / (tp + fp) if (tp + fp) else 0.0\n",
    "    rec  = tp / (tp + fn) if (tp + fn) else 0.0\n",
    "    return 0.0 if (prec + rec) == 0 else 2 * prec * rec / (prec + rec)\n",
    "\n",
    "best = None\n",
    "for ms in min_grid:\n",
    "    for eps in eps_grid:\n",
    "        lbls = DBSCAN(eps=eps, min_samples=ms, metric='precomputed').fit_predict(D)\n",
    "        f1 = f1_anom(lbls, y_true)\n",
    "        n_anom = int(np.sum(lbls == -1))\n",
    "        n_clusters = len(set(lbls) - {-1})\n",
    "        score = (f1, -abs(n_anom - int(np.sum(y_true == -1))), -n_clusters)\n",
    "        if (best is None) or (score > best[0]):\n",
    "            best = (score, ms, float(eps), lbls, f1, n_anom, n_clusters)\n",
    "\n",
    "_, best_ms, best_eps, labels, f1_best, n_anom, n_clusters = best\n",
    "\n",
    "print(f\"Escolha final -> eps={best_eps:.3f}, min_samples={best_ms} | clusters={n_clusters} | anomalias={n_anom} | F1_anom={f1_best:.3f}\")\n",
    "\n",
    "# 5) Plot: todas as séries (azul = normais, vermelho = anomalias detectadas)\n",
    "plt.figure(figsize=(12, 5))\n",
    "first_norm = True; first_anom = True\n",
    "for i, serie in enumerate(X):\n",
    "    if labels[i] == -1:\n",
    "        plt.plot(t, serie, c='red', alpha=0.9, lw=1.2, label='anomalia' if first_anom else None)\n",
    "        first_anom = False\n",
    "    else:\n",
    "        plt.plot(t, serie, c='blue', alpha=0.35, lw=0.9, label='normal' if first_norm else None)\n",
    "        first_norm = False\n",
    "plt.title(f\"DBSCAN + DTW | eps={best_eps:.3f}, min_samples={best_ms}\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
